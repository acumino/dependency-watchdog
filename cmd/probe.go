/*
SPDX-FileCopyrightText: 2019 SAP SE or an SAP affiliate company and Gardener contributors

SPDX-License-Identifier: Apache-2.0
*/

package cmd

import (
	"context"
	"os"

	"github.com/gardener/dependency-watchdog/pkg/scaler"
	scalerapi "github.com/gardener/dependency-watchdog/pkg/scaler/api"
	gardenerclientset "github.com/gardener/gardener/pkg/client/extensions/clientset/versioned"
	gardenerinformer "github.com/gardener/gardener/pkg/client/extensions/informers/externalversions"
	"github.com/spf13/cobra"
	"k8s.io/client-go/discovery/cached/memory"
	"k8s.io/client-go/dynamic"
	"k8s.io/client-go/informers"
	"k8s.io/client-go/kubernetes"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/restmapper"
	"k8s.io/client-go/scale"
	"k8s.io/client-go/tools/clientcmd"
	"k8s.io/client-go/tools/leaderelection"
	"k8s.io/client-go/tools/leaderelection/resourcelock"
	"k8s.io/klog"
)

// probeCmd represents the probe command
var probeCmd = &cobra.Command{
	Use:   "probe",
	Short: "Probe kubernetes apiservers for health and scale up/down dependant controllers.",
	Long: `Probe kubernetes apiservers for health and scale down the dependant controllers 
	if the apiserver is reachable internally and not reachable externally. It scales the
	dependent controllers back up when the apiserver becomes reachable externally.`,
	Run: runProbe,
}

func init() {
	rootCmd.AddCommand(probeCmd)
}

func runProbe(cmd *cobra.Command, args []string) {
	klog.V(5).Info("Running probe command")
	klog.V(2).Infoln("Running probe command with the following parameters:")
	klog.V(2).Infoln("config-file: ", configFile)
	klog.V(2).Infoln("kubeconfig: ", kubeconfig)
	klog.V(2).Infoln("master: ", deployedNamespace)
	klog.V(2).Infoln("deployed-namespace: ", masterURL)
	klog.V(2).Infoln("concurrent-syncs: ", concurrentSyncs)
	klog.V(2).Infoln("qps: ", qps)
	klog.V(2).Infoln("burst: ", burst)
	klog.V(2).Infoln("port: ", port)

	// set up signals so we handle the first shutdown signal gracefully
	stopCh := setupSignalHandler()
	deps, err := scaler.LoadProbeDependantsListFile(configFile)
	if err != nil {
		klog.Fatalf("Error parsing config file: %s", err.Error())
	}

	configContent, err := scalerapi.Encode(deps)
	klog.V(2).Infof("Probe configuration: \n %s", configContent)

	config, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)
	if err != nil {
		klog.Fatalf("Error parsing kubeconfig file: %s", err.Error())
	}

	config.QPS = qps
	config.Burst = burst

	clientset, err := kubernetes.NewForConfig(config)
	if err != nil {
		klog.Fatalf("Error creating k8s clientset: %s", err.Error())
	}

	mapper := restmapper.NewDeferredDiscoveryRESTMapper(memory.NewMemCacheClient(clientset.Discovery()))
	var opts []informers.SharedInformerOption
	if deps.Namespace != "" {
		opts = append(opts, informers.WithNamespace(deps.Namespace))
	}
	factory := informers.NewSharedInformerFactoryWithOptions(
		clientset,
		defaultSyncDuration,
		opts...)

	gardenerClientSet, err := gardenerclientset.NewForConfig(config)
	if err != nil {
		klog.Fatalf("Error creating k8s clientset: %s", err.Error())
	}

	gardenerInformerFactory := gardenerinformer.NewSharedInformerFactory(
		gardenerClientSet,
		defaultSyncDuration,
	)

	scaleKindResolver := scale.NewDiscoveryScaleKindResolver(clientset.Discovery()) // DiscoveryScaleKindResolver does the caching
	scaleGetter := scale.New(clientset.RESTClient(), mapper, dynamic.LegacyAPIPathResolverFunc, scaleKindResolver)
	controller := scaler.NewController(clientset, mapper, scaleGetter, factory, gardenerInformerFactory, deps, stopCh)
	leaderElectionClient := kubernetes.NewForConfigOrDie(rest.AddUserAgent(config, "dependency-watchdog-election"))
	recorder := createRecorder(leaderElectionClient)
	run := func(ctx context.Context) {
		go func() {
			err := serveMetrics()
			if err != nil {
				klog.Fatalf("Can't serve metrics: %s", err)
			}
		}()
		klog.Info("Starting endpoint controller.")
		if err = controller.Run(concurrentSyncs); err != nil {
			klog.Fatalf("Error running controller: %s", err.Error())
		}
		panic("unreachable")
	}
	if !*controller.LeaderElection.LeaderElect {
		run(nil)
	}

	id, err := os.Hostname()
	if err != nil {
		klog.Fatalf("error fetching hostname: %v", err)
	}

	rl, err := resourcelock.New(controller.LeaderElection.ResourceLock,
		deployedNamespace,
		"dependency-watchdog-probe",
		leaderElectionClient.CoreV1(),
		leaderElectionClient.CoordinationV1(),
		resourcelock.ResourceLockConfig{
			Identity:      id,
			EventRecorder: recorder,
		})
	if err != nil {
		klog.Fatalf("error creating lock: %v", err)
	}

	ctx := context.TODO()
	leaderelection.RunOrDie(ctx, leaderelection.LeaderElectionConfig{
		Lock:          rl,
		LeaseDuration: controller.LeaderElection.LeaseDuration.Duration,
		RenewDeadline: controller.LeaderElection.RenewDeadline.Duration,
		RetryPeriod:   controller.LeaderElection.RetryPeriod.Duration,
		Callbacks: leaderelection.LeaderCallbacks{
			OnStartedLeading: run,
			OnStoppedLeading: func() {
				klog.Fatalf("leaderelection lost")
			},
		},
	})
	panic("unreachable")
}
